{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n这里的mark改了一下。。你对自己做的是35个单词一组。\\n\\n分散 103\\n集中 102\\n一次 101\\n206-240\\n\\n后面打的mark\\n 10 11\\n\\n  自信程度\\n\\n20 - 25\\n\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "这里的mark改了一下。。你对自己做的是35个单词一组。\n",
    "\n",
    "分散 103\n",
    "集中 102\n",
    "一次 101\n",
    "206-240\n",
    "\n",
    "后面打的mark\n",
    " 10 11\n",
    "\n",
    "  自信程度\n",
    "\n",
    "20 - 25pp\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from autoreject import AutoReject"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_ids = [\n",
    "            'prex006','prex009','prex010','prex011','prex012',\n",
    "                        'prex015','prex016','prex017','prex018','prex019','prex022','prex023',\n",
    "                        'prex026','prex027','prex028'\n",
    "                        ,'prex033','prex034','prex037','prex038','prex039','prex040',\n",
    "                        'prex041','prex042','prex043','prex044','prex045',\n",
    "                        'prex046','prex047','prex048','prex049','prex050'\n",
    "          ]\n",
    "\n",
    "sub_ids = [ 'prex006' ]\n",
    "evoked_original = list()\n",
    "evoked_clean = list()\n",
    "for sub_id,i in zip(sub_ids,range(len(sub_ids))):\n",
    "    #读入之前处理好的reconst_raw\n",
    "    save_path_ica = 'E:\\Python LYW\\RSA\\spaced_learning\\pre10\\preprocessing\\data\\\\4raw_ica\\\\'\n",
    "    reconst_raw=mne.io.read_raw_fif(save_path_ica + sub_id + '.fif')\n",
    "    #截取block0的时间段，每个被试不一样。。你要换个mark才行。\n",
    "    #或者有什么办法读取一个segment。\n",
    "    # reconst_raw = reconst_raw.crop(tmax = 515.480)\n",
    "    # reconst_raw = reconst_raw.crop(tmax = 1159.360)\n",
    "    #这个是被试测试一的。\n",
    "    #epoch\n",
    "    target_events = {\n",
    "                     'Stimulus/S101':101, 'Stimulus/S102':102, 'Stimulus/S103':103,\n",
    "                     'Stimulus/S 10':10,'Stimulus/S 11':11,'Stimulus/S 20':20 ,\n",
    "                     'Stimulus/S 21':21,'Stimulus/S 22':22,'Stimulus/S 23':23,\n",
    "                     'Stimulus/S 24':24,'Stimulus/S 25':25}\n",
    "\n",
    "    #先找events，epoch和average都是一步的事情。\n",
    "    custom_mapping = target_events\n",
    "    (events_from_annot,\n",
    "     event_dict) = mne.events_from_annotations(reconst_raw, event_id=custom_mapping)\n",
    "    #还不如直接对 events_from_annot 进行切片\n",
    "    #先遍历，把合适的单词筛序出来\n",
    "    #对应的vocabulary已经列出，然后怎么提取呢？\n",
    "\n",
    "    events=[]\n",
    "\n",
    "    voc_mark = 101\n",
    "        # print(voc_mark)\n",
    "        #遍历所有，再做一次筛选mark\n",
    "    for j_event in range(len(events_from_annot)-1):\n",
    "            # if ((events_from_annot[j_event,2] == voc_mark and events_from_annot[j_event+1,2] == 16)or(events_from_annot[j_event,2]==voc_mark and events_from_annot[j_event+1,2]==11)):\n",
    "            #全部叠加，不考虑是否能够recognize。\n",
    "        if events_from_annot[j_event,2] == voc_mark and events_from_annot[j_event+1,2] == 11 and events_from_annot[j_event+2,2]>=21:\n",
    "            events.append(events_from_annot[j_event,:])\n",
    "            print(events_from_annot[j_event,:])\n",
    "    count_remembered_events = len(events)\n",
    "    #取全部event对应的epochs\n",
    "    # _, eog_scores= icas[i].find_bads_eog(raws[i])\n",
    "    # ica.plot_scores(eog_scores)\n",
    "    #顺带对比ica前后的效果。\n",
    "    epochs = mne.Epochs(reconst_raw, events, baseline =(-0.2,0),tmin=-0.2, tmax=0.8, event_id=None,\n",
    "                        preload=True, event_repeated='merge')\n",
    "\n",
    "    save_path_epoch = 'E:\\Python LYW\\RSA\\spaced_learning\\pre10\\preprocessing\\data\\\\5epoch\\\\'\n",
    "    epochs.save(save_path_epoch + sub_id + '-epo.fif', overwrite=True)\n",
    "\n",
    "\n",
    "    #用autoreject处理一下\n",
    "    n_interpolates = np.array([1, 4, 32])\n",
    "    consensus_percs = np.linspace(0, 0.8, 11)\n",
    "    picks = mne.pick_types(epochs.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                           include=[], exclude=[])\n",
    "    ar = AutoReject(n_interpolates,consensus_percs,picks=picks,\n",
    "                    thresh_method='random_search',random_state=23)\n",
    "    ar.fit(epochs)\n",
    "    epochs_clean = ar.transform(epochs)\n",
    "\n",
    "\n",
    "    #先average看下结果，对比\n",
    "    #如果结果不错就可以分别保存下来。\n",
    "    evoked_original.append(epochs.average())\n",
    "    evoked_clean.append(epochs_clean.average())\n",
    "\n",
    "    save_path_epoch_clean = 'E:\\Python LYW\\RSA\\spaced_learning\\pre10\\preprocessing\\data\\\\6epoch_clean\\\\'\n",
    "    epochs_clean.save(save_path_epoch_clean + sub_id + '-epo.fif', overwrite=True)\n",
    "    save_path_evoked = 'E:\\Python LYW\\RSA\\spaced_learning\\pre10\\preprocessing\\data\\\\7evoked\\\\'\n",
    "    #为什么这里加i?因为是list，epochs不是list\n",
    "    evoked_clean[i].save(save_path_evoked + sub_id + '-ave.fif')\n",
    "\n",
    "    # fig_original = evoked_original[i].plot(window_title = sub_id)\n",
    "    # fig_original.savefig(save_path_evoked+'_original_' + sub_id)\n",
    "    # fig_clean = evoked_clean[i].plot(window_title = sub_id)\n",
    "    # fig_clean.savefig(save_path_evoked+'_clean_'+ sub_id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_original = evoked_original[i].plot(window_title = sub_id)\n",
    "fig_original.savefig(save_path_evoked+'_original_' + sub_id)\n",
    "fig_clean = evoked_clean[i].plot(window_title = sub_id)\n",
    "fig_clean.savefig(save_path_evoked+'_clean_'+ sub_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evoked_clean.plot()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "mne",
   "language": "python",
   "display_name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}